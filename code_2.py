# -*- coding: utf-8 -*-
"""Code_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wnJKs7caBQz-I5tzjLI0_k8w8W4oUks-
"""

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import SGD

import matplotlib.pyplot as plt
import numpy as np

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize the pixel values to [0, 1]
x_train, x_test = x_train / 255.0, x_test / 255.0

# Flatten the images to 1D array of 784 features (28*28)
x_train = x_train.reshape((x_train.shape[0], 784))
x_test = x_test.reshape((x_test.shape[0], 784))

model = Sequential([
    Dense(300, activation='tanh', input_shape=(784,)),
    Dense(10, activation='softmax')
])

model.compile(optimizer=SGD(learning_rate=0.1),  # Using SGD optimizer
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Mini-batch gradient descent with a batch size of 50
history = model.fit(x_train, y_train, epochs=10, batch_size=50, validation_data=(x_test, y_test))

test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print('\nTest accuracy:', test_acc)

# Plot learning curves
plt.figure(figsize=(5, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Negative Log Probability')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

predictions = model.predict(x_test)
predicted_classes = np.argmax(predictions, axis=1)

correct_indices = np.nonzero(predicted_classes == y_test)[0]
incorrect_indices = np.nonzero(predicted_classes != y_test)[0]

plt.figure(figsize=(10, 5))
for i, correct in enumerate(correct_indices[:20]):
    plt.subplot(4, 5, i + 1)
    plt.imshow(x_test[correct].reshape(28, 28), cmap='gray', interpolation='none')
    plt.title("Predicted {}, Class {}".format(predicted_classes[correct], y_test[correct]))
    plt.xticks([])
    plt.yticks([])

plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
for i, incorrect in enumerate(incorrect_indices[:10]):
    plt.subplot(2, 5, i + 1)
    plt.imshow(x_test[incorrect].reshape(28, 28), cmap='gray', interpolation='none')
    plt.title("Pred {}, True {}".format(predicted_classes[incorrect], y_test[incorrect]))
    plt.xticks([])
    plt.yticks([])

plt.tight_layout()
plt.show()

weights = model.layers[0].get_weights()[0]  # This gets the weights from the input layer to the first hidden layer

# Assuming weights shape is (784, 300)
# Select two neurons, e.g., the first and the last neuron in the hidden layer
w1 = weights[:, 0].reshape(28, 28)  # Weights for neuron 1
w2 = weights[:, -1].reshape(28, 28) # Weights for neuron 300

plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
plt.imshow(w1, cmap='viridis', interpolation='none')
plt.title('Weights: Neuron 1')
plt.colorbar()
plt.subplot(1, 2, 2)
plt.imshow(w2, cmap='viridis', interpolation='none')
plt.title('Weights: Neuron 300')
plt.colorbar()
plt.show()